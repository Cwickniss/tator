domain: www.yourdomain.com
# Requests for these domains will be redirected to the actual domain.
redirects:
  - other.domain.com
  - other.domain1.com
# Enable this to turn on "down for maintenance" page.
maintenance: false
nginxReplicas: 5
gunicornReplicas: 20
djangoSecretKey: "<Your django secret key>"
postgresHost: "<Your RDS DB identifier>.clsla2zlnxez.<Your aws region>.rds.amazonaws.com"
postgresUsername: "<Your postgres username>"
postgresPassword: "<Your postgres password>"
redisHost: "<Your ElastiCache endpoint>"
elasticsearchHost: "https://<Your Amazon Elasticsearch Service VPC endpoint>"
dockerUsername: "<Your ECR username>"
dockerPassword: "<Your ECR password>"
dockerRegistry: "<Your aws account ID>.dkr.ecr.<Your aws region>.amazonaws.com"
sslBundle: |
  -----BEGIN CERTIFICATE-----
  <Insert certificate here>
  -----END CERTIFICATE-----
  -----BEGIN CERTIFICATE-----
  <Insert certificate here>
  -----END CERTIFICATE-----
sslKey: |
  -----BEGIN PRIVATE KEY-----
  <Insert private key here>
  -----END PRIVATE KEY-----
pv:
  enabled: true
  nfsServer: "<Your efs filesystem ID>.efs.<Your aws region>.amazonaws.com"
  nfsMountOptions:
    - nfsvers=4.1
    - rsize=1048576
    - wsize=1048576
    - hard
    - timeo=600
    - retrans=2
    - noresvport
  staticPath: "/static"
  uploadPath: "/upload"
  mediaPath: "/media"
  rawPath: "/raw"
  backupPath: "/backup"
  migrationsPath: "/migrations"
pvc:
  # These are required to be valid values but will be ignored by AWS EFS.
  # EFS will not limit the amount of storage you can use.
  staticSize: 1Gi
  uploadSize: 1Ti
  mediaSize: 4Ti
  rawSize: 6Ti
  backupSize: 100Gi
  migrationsSize: 1Gi
hpa:
  nginxMinReplicas: 2
  nginxMaxReplicas: 10
  nginxCpuPercent: 50
  gunicornMinReplicas: 4
  gunicornMaxReplicas: 10
  gunicornCpuPercent: 50
  daphneMinReplicas: 2
  daphneMaxReplicas: 10
  daphneCpuPercent: 50
  tusdMinReplicas: 2
  tusdMaxReplicas: 10
  tusdCpuPercent: 50
metallb:
  # A load balancer implementation is provided by AWS.
  enabled: false
postgis:
  enabled: false
redis:
  # Enable this to use the Redis helm chart installed as a dependency
  # instead of AWS Elasticache.
  enabled: false
metrics-server:
  # AWS has its own metrics for monitoring but this is still needed for
  # the horizontal pod autoscaler (HPA).
  enabled: true
  args:
    - --v=2
    - --kubelet-insecure-tls=true
    - --kubelet-preferred-address-types=InternalIP
elasticsearch:
  # Enable this to use the Elasticsearch helm chart installed as a
  # dependency instead of AWS Elasticsearch Service.
  enabled: false
filebeat:
  enabled: true
  image: docker.elastic.co/beats/filebeat-oss
  imageTag: 7.4.2
  extraEnvs:
  - name: ELASTICSEARCH_HOSTS
    value: "https://<Your Amazon Elasticsearch Service VPC endpoint>"
  filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: docker
        containers.ids:
        - '*'
        processors:
        - add_kubernetes_metadata:
            in_cluster: true
      output.elasticsearch:
        hosts: '${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}'
      setup.ilm.enabled: false
kibana:
  enabled: true
  image: docker.elastic.co/kibana/kibana-oss
  imageTag: 7.4.2
  kibanaConfig:
    kibana.yml: |
      server:
        basePath: /logs
  elasticsearchHosts: "https://<Your Amazon Elasticsearch Service VPC endpoint>"
cluster-autoscaler:
  enabled: true
awsStorage:
  # Enable this if you are using EFS storage.
  enabled: true
  # EFS filesystem ID.
  filesystemId: <Your filesystem ID>
awsFargate:
  # Enable this if you are using fargate for containers.
  enabled: true
remoteTranscodes:
  # Enable this if you would like to do transcodes with a different
  # Kubernetes cluster, such as an on-premises cluster. Follow instructions
  # at doc/job-cluster.md to set up the cluster.
  enabled: true
  # Host/port are obtained via the following (run on the transcode cluster):
  #   echo $(kubectl config view --minify | grep server | cut -f 2- -d ":" | tr -d " ")
  host: "your.transcode.domain.org"
  port: "6443"
  # Token can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d " ")
  #   echo $TOKEN
  token: "Bearer <Your token here>"
  # Certificate can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   CERT=$(kubectl get secret $SECRET_NAME -o yaml | grep -E '^  ca.crt' | cut -f2 -d':' | tr -d " ")
  #   echo $CERT | base64 --decode
  cert: |
    -----BEGIN CERTIFICATE-----
    <Insert certificate here>
    -----END CERTIFICATE-----

